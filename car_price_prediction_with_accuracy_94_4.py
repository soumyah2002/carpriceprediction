# -*- coding: utf-8 -*-
"""car-price-prediction-with-accuracy-94-4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RWPJqKMwkRsuTw4x1NyambY7JokaOihy

# <center>Car Price Prediction </center>

# Outlines 
> Project target : Predict Used car price based on car specifications 
1. Data cleaning 
2. feature engineering 
3. Getting more insights 
4. Data Pre-processing
5. modeling 
6. evalution
"""

from google.colab import drive
drive.mount('/content/drive')

pip install --upgrade category_encoders

# Commented out IPython magic to ensure Python compatibility.
# Packages for EDA 
import pandas as pd 
import numpy as np 
import seaborn as sns 

# Data Preprocessing
from sklearn.model_selection import train_test_split
import category_encoders as ce
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import PowerTransformer

# Modeling
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LassoCV
from sklearn.linear_model import RidgeCV
from sklearn.linear_model import ElasticNetCV
np.seterr(divide='ignore', invalid='ignore', over='ignore')


# Showing Plots inside notebook
# %matplotlib inline  
sns.set(rc={'figure.figsize': [7, 14]}, font_scale=1.2) # Standard figure size for all

df = pd.read_csv("/content/drive/MyDrive/colab/train-data.csv")
df.sample(5)

"""# 1. Data cleaning

### 1.1 Investegation
"""

df.info()

df.describe()

df.duplicated().sum()

"""#### Notes 
- Numerical values mixed with text in (Mileage, Engine, Power) columns. 
    - in Milage column we have 2 units (kmpl & km/kg)
- Most of New_Price Column is null values (so, we have 2 solution.) 
    - remove it or scrap some data to fill it. 
- Null values values in other columns 
- Seats columns have some values with Zero !! 
- duplicated rows founded 
- Power column have values "null bhp"

###  1.2 Working with data issues

> Dropping duplicated columns
"""

# Remove duplicated row 
df.drop_duplicates(inplace=True)

"""> Drop "New_Price" column because most of them is null 
- Another solution is to Scrap New price 
"""

df.drop("New_Price", axis=1, inplace=True)

"""> Imputing Null values """

# My solution 
from sklearn.impute import SimpleImputer

null_col = ['Mileage', 'Engine' , 'Power','Seats']

imputer = SimpleImputer(strategy='most_frequent')
df[null_col] = imputer.fit_transform(df[null_col])

"""> Make sure that all data type is correct """

df = df.convert_dtypes()

"""> Impute zeros in seats column """

df['Seats'].replace(to_replace=0 ,value= df['Seats'].mode()[0],inplace=True)

df[df['Power'] == 'null bhp']

indx = df[df['Power'] == 'null bhp'].index

df.drop(indx,axis=0,inplace=True)

"""- (KMPL) is refered to Kilometers Per Litre
- (km/kg) is refered to kilometers Per kilogram
> 1 liter  = 1 kilogram 
- Reference : https://en.wikipedia.org/wiki/Litre
"""

dirty_cols = ['Mileage', 'Engine', 'Power']

for col in dirty_cols:
    df[col] = df[col].apply(lambda x : float(x.split()[0]))

"""#### values health checking """

df.sample(5)

df.info()

"""___________________________________

# 2. Feature Engineering
"""

# Check Name column uniques 
df["Name"].nunique()

print("Percentage of uniques",round(df["Name"].nunique()/df.shape[0]*100),"%")

""">  "Name" feature has no affect that's because it has so many unique values 
- So let's make it useful and impactful 
"""

df["Name"]

"""> We can notice that the first word of the name is (Brand), so let's get it """

df["Brand"] = df["Name"].apply(lambda x : x.split()[0])

df["Brand"].nunique()

"""- A huge difference here, From this columns we can make a big affect.

> Another observation that first two word can express wich car we want. 
- So, let's change name column with just first 2 words.
"""

df["Name"] = df["Name"].apply(lambda x : " ".join(x.split()[:2]))

df["Name"].nunique()

"""- That's great, Now we can make an affect with name column. """

df = df.convert_dtypes()

"""________________________"""

df.sample(5)

df.info()

df.describe()

"""________________________"""

df.to_csv("Cleaned_Data.csv")

"""# 3. Data understanding

### 3.1 Univariate analysis
"""

sns.histplot(data = df , x = 'Kilometers_Driven');

sns.histplot(data = df , x = 'Mileage');

sns.histplot(data = df , x = 'Engine');

sns.histplot(data = df , x = 'Power');

sns.histplot(data = df , x = 'Price');

numerical_cols = ['Kilometers_Driven' , 'Mileage' , 'Engine' , 'Power','Price']

pip install datasist

df_nums = df.copy()
from datasist.structdata import detect_outliers 

outliears = detect_outliers(df_nums[numerical_cols],0,df_nums[numerical_cols].columns)
df_nums.drop(outliears,inplace=True)

# outliears = detect_outliers(df[['Price']],0,df[['Price']].columns)
# df[['Price']].drop(outliears,inplace=True)

sns.histplot(data = df_nums , x = 'Kilometers_Driven');

sns.histplot(data = df_nums , x = 'Mileage');

sns.histplot(data = df_nums , x = 'Engine');

sns.histplot(data = df_nums , x = 'Power');

sns.histplot(data = df_nums , x = 'Price');

"""## Data summery : 
    - Numerical columns follow the gussian distribution, but have Outliers 
    (My desicion is not to remove it,because it will ne useful)
    - The countrt with the most sales spread is `Mumbai` and the least is `Ahmedabad`
    - Most of cars is form 2010 and 2015
"""

df

"""________________________

# 4. Data pre-processing

### 4.1 Data transformation

#### 4.1.1 Catogerical transformation

##### 4.1.1.1 ordinal transformation
"""

transformation = {
    "First":3,
    "Second":2,
    "Third":1,
    "Fourth & Above":0
}

df['Owner_Type'] = df['Owner_Type'].map(transformation)

"""> The best case of owner type is (First), Of course the worst is Fourth & Above

##### 4.1.1.2 Nominal transformation
"""

Nominal_data = ['Name','Location','Fuel_Type','Transmission','Brand']
binaryencoder = ce.BinaryEncoder(cols=Nominal_data)
df = binaryencoder.fit_transform(df)

Numerical_data = ['Year','Kilometers_Driven','Mileage','Engine','Power','Seats','Price']

"""#### 4.2.1 Numerical transformation """

# power = PowerTransformer()
new_df = pd.DataFrame(PowerTransformer().fit_transform(df), columns=df.columns, index=df.index)

"""### 4.1 Data splitting """

# define dataset
X, y = new_df.drop("Price",axis=1) , new_df["Price"] 

# split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

"""# 5.0 Modeling

### 5.1 Linear regression (OLS method)
"""

reg = LinearRegression().fit(X_train,(y_train))
print("R-Squered Trian",round((reg.score(X_train,  (y_train))*100),2),'%')
print("R-Squered Test",round((reg.score(X_test,  (y_test))*100),2),'%')
y_pred = reg.predict(X_test)
print(f"RMSE: {mean_squared_error(y_pred,(y_test))}")

"""### 5.3 Linear regression (Lasso method)"""

from sklearn.preprocessing import PolynomialFeatures
poly_reg = PolynomialFeatures(degree = 2)
poly_reg.fit(X_train)

X_train = poly_reg.transform(X_train)
X_test = poly_reg.transform(X_test)

reg = LassoCV().fit(X_train,(y_train))
print("R-Squered Trian",round((reg.score(X_train,  (y_train))*100),2),'%')
print("R-Squered Test",round((reg.score(X_test,  (y_test))*100),2),'%')
y_pred = reg.predict(X_test)
print(f"RMSE: {mean_squared_error(y_pred,(y_test))}")

"""### 5.4 Linear regression (Ridge method)"""

reg = RidgeCV().fit(X_train,(y_train))
print("R-Squered Trian",round((reg.score(X_train,  (y_train))*100),2),'%')
print("R-Squered Test",round((reg.score(X_test,  (y_test))*100),2),'%')
y_pred = reg.predict(X_test)
print(f"RMSE: {mean_squared_error(y_pred,(y_test))}")

"""### 5.4 Linear regression (ElasticNetCV method)"""

reg = ElasticNetCV().fit(X_train,(y_train))
print("R-Squered Trian",round((reg.score(X_train,  (y_train))*100),2),'%')
print("R-Squered Test",round((reg.score(X_test,  (y_test))*100),2),'%')
y_pred = reg.predict(X_test)
print(f"RMSE: {mean_squared_error(y_pred,(y_test))}")

"""> The best reqularization method That fit data well __ElasticNet__"""